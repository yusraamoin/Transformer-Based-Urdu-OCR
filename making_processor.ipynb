{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Samreenhabib/Urdu-OCR/blob/main/making_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_rmekvjh-omy"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "image_processor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-384\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "\n",
    "#processor = TrOCRProcessor(feature_extractor=feature_extractor, tokenizer=decoder_tokenizer)\n",
    "processor = TrOCRProcessor(image_processor=image_processor, tokenizer=decoder_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PYBr94va_jv0"
   },
   "outputs": [],
   "source": [
    "# processor.save_pretrained('../test/test_processor')\n",
    "processor.save_pretrained('./preprocessor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfqpUIo15AS4",
    "outputId": "f521f9a8-0aa8-46a0-b0cd-31a2c24a1a8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>فہرست اداریہ ڈاکٹر ثروت رضوی ۴ اقبال کی شاعران...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>جمیل یوسف ۵ ———— پروین شاکر کی یاد میں شاہدہ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>حسن ۱۲ شاعری میں تانیثیت کے منظرنامے پر دستخط صدف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>مرزا ۱۹ پروین شاکر رضیہ سبحان ۳۷ عکسِ خوشبو ہوں—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>پروین شاکر ڈاکٹر نزہت عباسی ۴۱ پروین شاکر کی ش...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename                                               text\n",
       "0    0.png  فہرست اداریہ ڈاکٹر ثروت رضوی ۴ اقبال کی شاعران...\n",
       "1    1.png       جمیل یوسف ۵ ———— پروین شاکر کی یاد میں شاہدہ\n",
       "2    2.png  حسن ۱۲ شاعری میں تانیثیت کے منظرنامے پر دستخط صدف\n",
       "3    3.png   مرزا ۱۹ پروین شاکر رضیہ سبحان ۳۷ عکسِ خوشبو ہوں—\n",
       "4    4.png  پروین شاکر ڈاکٹر نزہت عباسی ۴۱ پروین شاکر کی ش..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "df = pd.read_excel(\"sen_dataset.xlsx\", header=None)\n",
    "df.rename(columns={0: \"filename\", 1: \"text\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "L4sMaHuQ5gTV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3)\n",
    "\n",
    "# train_df.dropna()\n",
    "# test_df.dropna()\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1948.png</td>\n",
       "      <td>اور مشورے جو ترقی پسند ادیبوں اورتحریک کے لیے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5143.png</td>\n",
       "      <td>سے مالامال ہے۔ یہاں مختلف نسلوں اور رنگوں کے لوگ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228.png</td>\n",
       "      <td>طرح اور شعرا بھی اس کیفیت سے گزرے ہوں گے۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002.png</td>\n",
       "      <td>ڈرامے میں شریک تھے۔ اسوقت کسی کے سامنے سیاست یا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576.png</td>\n",
       "      <td>ہیں۔ اس کا اندازہ ان کی وفات سے قبل کہے</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                                               text\n",
       "0  1948.png  اور مشورے جو ترقی پسند ادیبوں اورتحریک کے لیے ...\n",
       "1  5143.png   سے مالامال ہے۔ یہاں مختلف نسلوں اور رنگوں کے لوگ\n",
       "2   228.png          طرح اور شعرا بھی اس کیفیت سے گزرے ہوں گے۔\n",
       "3  2002.png    ڈرامے میں شریک تھے۔ اسوقت کسی کے سامنے سیاست یا\n",
       "4   576.png            ہیں۔ اس کا اندازہ ان کی وفات سے قبل کہے"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6hyna5wS5hX4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['filename'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, padding=\"max_length\",max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DMa6KXH15wsx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"./preprocessor/\")\n",
    "\n",
    "train_dataset = IAMDataset(root_dir= \"C:/Working_Directory/NCL/Urdu_OCR/FYP/NMT - FYP/images/\",\n",
    "                           df = train_df,\n",
    "                           processor=processor)\n",
    "\n",
    "eval_dataset = IAMDataset(root_dir= \"C:/Working_Directory/NCL/Urdu_OCR/FYP/NMT - FYP/images/\",\n",
    "                           df = test_df,\n",
    "                           processor = processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zS0V-GTS6YkT",
    "outputId": "295fe667-b17a-498e-e49c-12df1bf6ade1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4550\n",
      "Number of validation examples: 1951\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 'labels': tensor([    0,   336,   584,  5782, 36689,   550,   418,   293,   374,   292,\n",
       "         13145,  2121, 11656,   318,  6620,   808,  8257,     2,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c69U-3YH6cMF",
    "outputId": "7548d140-bb31-4d81-a6a3-dd06b45d70d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[5]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "id": "tRgMIlp06dA-",
    "outputId": "dde67c5a-8005-4d4a-f794-5c21974faab0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=691x58>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['filename'][5]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viLah64h6sW1",
    "outputId": "857a2e73-8b1e-442e-f99d-4a268ffd9995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حصہ بنا لے جب کہ استعماریت(Colonialism) سے مراد ایک طاقت\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vrJ3ojeJA6n9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-384 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at urduhack/roberta-urdu-small and are newly initialized: ['roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\"google/vit-base-patch16-384\", \n",
    "                                                                  \"urduhack/roberta-urdu-small\")\n",
    "\n",
    "# set decoder config to causal lm\n",
    "model.config.decoder.is_decoder = True\n",
    "model.config.decoder.add_cross_attention = True\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-_bwRqJFvDH"
   },
   "source": [
    "using steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_otg0vO8BOU3"
   },
   "outputs": [],
   "source": [
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     fp16=False, \n",
    "#     output_dir=\"./\",\n",
    "#     logging_steps=50,\n",
    "#     save_steps=20,\n",
    "#     eval_steps=20,\n",
    "#     num_train_epochs=40,  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz9lc08aF7pr"
   },
   "source": [
    "evaluating using epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1-AHdsYMFe__"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   evaluation_strategy = \"epoch\",\n",
    "   learning_rate= 0.002,\n",
    "   per_device_train_batch_size = 32,\n",
    "   per_device_eval_batch_size = 8,\n",
    "   weight_decay = 0.01,\n",
    "   save_total_limit = 2,\n",
    "   num_train_epochs = 40,\n",
    "   output_dir = \"./train/\",\n",
    "   predict_with_generate=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-UNPv-KiBtvp"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load('cer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "vOYddNXnBxr2"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'filename' not found in the Excel file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from openpyxl import load_workbook\n",
    "\n",
    "# def compare_images_with_excel(file_path, folder_path, column_name):\n",
    "#     # Load the Excel file\n",
    "#     workbook = load_workbook(file_path)\n",
    "#     sheet = workbook.active\n",
    "    \n",
    "#     # Get the column index based on the column name\n",
    "#     column_index = None\n",
    "#     for cell in sheet[1]:\n",
    "#         if cell.value == column_name:\n",
    "#             column_index = cell.column_letter\n",
    "#             break\n",
    "    \n",
    "#     # Check if the column was found\n",
    "#     if column_index is None:\n",
    "#         print(f\"Column '{column_name}' not found in the Excel file.\")\n",
    "#         return\n",
    "    \n",
    "#     # Iterate over the values in the specified column\n",
    "#     for cell in sheet[column_index][1:]:\n",
    "#         image_filename = cell.value\n",
    "        \n",
    "#         # Construct the full image path\n",
    "#         image_path = os.path.join(folder_path, image_filename)\n",
    "        \n",
    "#         # Check if the image file exists\n",
    "#         if os.path.isfile(image_path):\n",
    "#             print(f\"Image '{image_filename}' found in the folder.\")\n",
    "#             folder.append(image_filename)\n",
    "#         else:\n",
    "#             print(f\"Image '{image_filename}' not found in the folder.\")\n",
    "    \n",
    "#     # Close the workbook\n",
    "#     workbook.close()\n",
    "\n",
    "# # Example usage\n",
    "# excel_file_path = \"sen_dataset.xlsx\"\n",
    "# image_folder_path = \"C:/Working_Directory/NCL/Urdu_OCR/FYP/NMT - FYP/images\"\n",
    "# column_name = \"filename\"\n",
    "# folder = []\n",
    "\n",
    "# compare_images_with_excel(excel_file_path, image_folder_path, column_name)\n",
    "# len(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def print_files_in_folder(folder_path):\n",
    "#     # Get a list of all files in the folder\n",
    "#     files = os.listdir(folder_path)\n",
    "    \n",
    "#     # Iterate over the files and print their names\n",
    "#     for file_name in files:\n",
    "#         # Construct the full file path\n",
    "#         file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "#         # Check if the file is a regular file\n",
    "#         if os.path.isfile(file_path):\n",
    "#             print(file_name)\n",
    "#             folder.append(file_name)\n",
    "\n",
    "# # Example usage\n",
    "# folder_path = \"C:/Working_Directory/NCL/Urdu_OCR/FYP/NMT - FYP/images\"\n",
    "# folder = []\n",
    "# print_files_in_folder(folder_path)\n",
    "# len(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "8k_Ufd6HB1Px",
    "outputId": "b353a36c-4a7f-4ac5-fc74-adc684c27e9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\danis\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4550\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5720\n",
      "  Number of trainable parameters = 241079584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='5720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  82/5720 34:03 < 40:00:38, 0.04 it/s, Epoch 0.57/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# instantiate trainer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      5\u001b[0m                         model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m      6\u001b[0m                         tokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mfeature_extractor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m                         data_collator \u001b[38;5;241m=\u001b[39m default_data_collator\n\u001b[0;32m     12\u001b[0m                         )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1791\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1794\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1797\u001b[0m ):\n\u001b[0;32m   1798\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1799\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\transformers\\trainer.py:2557\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2555\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2557\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\easyocr-env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "                        model = model,\n",
    "                        tokenizer = processor.feature_extractor,\n",
    "                        args = training_args,\n",
    "                        compute_metrics = compute_metrics,\n",
    "                        train_dataset = train_dataset,\n",
    "                        eval_dataset = eval_dataset,\n",
    "                        data_collator = default_data_collator\n",
    "                        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3J3XGY6GfkD",
    "outputId": "222ddded-d50a-4665-a17d-e368098cd42e"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('./trainings/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jzfohl8wG2J2"
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"./trainings/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okXisArpHLzQ"
   },
   "outputs": [],
   "source": [
    "image = Image.open('/content/6.png').convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5MR7dxhHR7W",
    "outputId": "7f68d1ae-71e9-425f-cac1-a427faaba681"
   },
   "outputs": [],
   "source": [
    "pixel_values = processor.feature_extractor(image, return_tensors=\"pt\").pixel_values \n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e4ruhWiHp4-",
    "outputId": "d380e4cd-9e85-4285-8ae1-51a9a682acd7"
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHTWc4xjd3_b",
    "outputId": "38f9d4a8-2f5b-4f53-8c42-c093c3cad65e"
   },
   "outputs": [],
   "source": [
    "test = Image.open('/content/8.png').convert(\"RGB\")\n",
    "testvalues = processor.feature_extractor(test, return_tensors=\"pt\").pixel_values \n",
    "print(testvalues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pHS_LzceBgO",
    "outputId": "e422ed6d-cc96-49b0-9d3c-2582faf0e03e"
   },
   "outputs": [],
   "source": [
    "testids = model.generate(testvalues)\n",
    "testtext = processor.batch_decode(testids, skip_special_tokens=True)[0]\n",
    "print(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0Ion1FGgkKT7q480ryU2r",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "making_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "easyocr-env",
   "language": "python",
   "name": "easyocr-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
